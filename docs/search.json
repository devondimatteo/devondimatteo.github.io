[
  {
    "objectID": "basic-example.html",
    "href": "basic-example.html",
    "title": "Habbit",
    "section": "",
    "text": "(tab content) 1\n\n\n\n(tab content) 2\n\nlibrary(tidyverse)\nggplot(mpg) +\n  geom_boxplot(aes(x = class, y = hwy))"
  },
  {
    "objectID": "basic-example.html#section-x",
    "href": "basic-example.html#section-x",
    "title": "Habbit",
    "section": "0.2 Section X",
    "text": "0.2 Section X\nThis is my introduction.\n\nx <- 1\nprint(x)\n\n[1] 1"
  },
  {
    "objectID": "basic-example.html#section-1.1",
    "href": "basic-example.html#section-1.1",
    "title": "Habbit",
    "section": "1.1 Section 1.1",
    "text": "1.1 Section 1.1\nDrink water"
  },
  {
    "objectID": "basic-example.html#section-1.2",
    "href": "basic-example.html#section-1.2",
    "title": "Habbit",
    "section": "1.2 Section 1.2",
    "text": "1.2 Section 1.2\nSleep again"
  },
  {
    "objectID": "hw0.html",
    "href": "hw0.html",
    "title": "Homework 0",
    "section": "",
    "text": "The following boxplot shows how the distribution of city MPG (cty) varies by a type of cars (class).\n\nggplot(mpg) +\n  geom_boxplot(aes(x = class, y = cty, fill = class))"
  },
  {
    "objectID": "hw1.html",
    "href": "hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "The following boxplot shows how the distribution of city MPG (cty) varies by a type of cars (class).\n\nggplot(mpg) +\n  geom_boxplot(aes(x = class, y = cty, fill = class))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Devon DiMatteo",
    "section": "",
    "text": "I am a student at State University of New York at Geneseo.\nI am interested in Data Analytics, Sustainability, and Economics\nI enjoy Music and Nature!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Devon DiMatteo",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo, 2023 - B.A./B.S. in Economics - Data Analytics"
  },
  {
    "objectID": "teamproj0.html",
    "href": "teamproj0.html",
    "title": "Team Project",
    "section": "",
    "text": "This is an example of the team project web-page."
  },
  {
    "objectID": "teamproj0.html#us-county-map-of-climate-change-belief-in-2018",
    "href": "teamproj0.html#us-county-map-of-climate-change-belief-in-2018",
    "title": "Team Project",
    "section": "1 US county map of climate change belief in 2018",
    "text": "1 US county map of climate change belief in 2018\nÂ  Estimated percentage who think that global warming is caused mostly by human activities\n\ncc_belief_county <- read_csv(\n  'https://bcdanl.github.io/data/cc_belief_county.csv'\n)\n\n\nlegend_title <- \"\"\n\nggplot(data = cc_belief_county) +\n  geom_polygon(aes(long, lat, group = group, \n                   fill = human2018),\n               color = \"grey\", size = 0.1) +\n  coord_map(\"bonne\", parameters = 41.6) + \n  scale_fill_gradient(legend_title, \n                      low='#0057e7', \n                      high='#d62d20') +\n  theme_map() + \n  theme(legend.position=\"right\") +\n  labs(caption = \"Data source: Yale Climate Change Communication\")"
  },
  {
    "objectID": "Data Cleaning Econ399.html",
    "href": "Data Cleaning Econ399.html",
    "title": "Data Cleaning Econ399",
    "section": "",
    "text": "R_Data_Econ399 <- read_excel(\"C:\\\\Users\\\\Devon DiMatteo\\\\Documents\\\\R Data Econ399.xlsx\")\nView(R_Data_Econ399)\n\n\ndf <- R_Data_Econ399 \ndf_1 <- df[, 1:2]\ndf_2 <- df[, 3:4]\ndf_3 <- df[, 5:6]\ndf_4 <- df[, 7:8]\ndf_5 <- df[, 9:10]\ndf_6 <- df[, 11:12]\ndf_7 <- df[, 13:14]\ndf_8 <- df[, 15:16]\ndf_9 <- df[, 17:18]\ndf_10 <- df[, 19:20]\ndf_11 <- df[, 21:22]\n\ndf_1$type <- colnames(df)[2]\ndf_2$type <- colnames(df)[4]\ndf_3$type <- colnames(df)[6]\ndf_4$type <- colnames(df)[8]\ndf_5$type <- colnames(df)[10]\ndf_6$type <- colnames(df)[12]\ndf_7$type <- colnames(df)[14]\ndf_8$type <- colnames(df)[16]\ndf_9$type <- colnames(df)[18]\ndf_10$type <- colnames(df)[20]\ndf_11$type <- colnames(df)[22]\n\ncolnames(df_1) <- c('country', 'value', 'type')\ncolnames(df_2) <- c('country', 'value', 'type')\ncolnames(df_3) <- c('country', 'value', 'type')\ncolnames(df_4) <- c('country', 'value', 'type')\ncolnames(df_5) <- c('country', 'value', 'type')\ncolnames(df_6) <- c('country', 'value', 'type')\ncolnames(df_7) <- c('country', 'value', 'type')\ncolnames(df_8) <- c('country', 'value', 'type')\ncolnames(df_9) <- c('country', 'value', 'type')\ncolnames(df_10) <- c('country', 'value', 'type')\ncolnames(df_11) <- c('country', 'value', 'type')\n\ndf_full <- rbind(df_1, df_2, df_3, df_4, df_5, \n                 df_6, df_7, df_8, df_9, df_10, df_11) %>% \n  arrange(country, type)\n\ndf_full_sum <- df_full %>% \n  group_by(country, type) %>% \n  count() %>% \n  arrange(-n)\n\n\n\ndf_wide <- df_full %>% \n  filter(!is.na(country)) %>% \n  pivot_wider(names_from = type,\n              values_from = value,\n              values_fill = NA)\n\n\n#df_final <- df_wide\n#write_csv(df_final, \"C:\\\\Users\\\\Devon DiMatteo\\\\Documents\\\\df_wide.csv\")\n\n\nsummary(df_wide)\n\n   country                AE                AG                CO2         \n Length:260         Min.   :  6.707   Min.   : 0.01528   Min.   : 0.0357  \n Class :character   1st Qu.: 82.649   1st Qu.: 2.18941   1st Qu.: 0.8076  \n Mode  :character   Median : 99.710   Median : 6.49978   Median : 3.0238  \n                    Mean   : 85.936   Mean   : 9.88120   Mean   : 4.1217  \n                    3rd Qu.:100.000   3rd Qu.:16.46574   3rd Qu.: 5.8907  \n                    Max.   :100.000   Max.   :58.15447   Max.   :32.7618  \n                    NA's   :1         NA's   :22         NA's   :25       \n       EX                GDP               I                 ME        \n Min.   :  0.6306   Min.   :   217   Min.   :-16.904   Min.   :0.1394  \n 1st Qu.: 23.0474   1st Qu.:  2251   1st Qu.: -0.440   1st Qu.:1.1586  \n Median : 34.0931   Median :  6897   Median :  2.089   Median :1.6042  \n Mean   : 40.7854   Mean   : 17269   Mean   :  3.111   Mean   :1.8659  \n 3rd Qu.: 49.9186   3rd Qu.: 19542   3rd Qu.:  4.974   3rd Qu.:2.2215  \n Max.   :204.3208   Max.   :199377   Max.   :143.534   Max.   :8.6377  \n NA's   :33         NA's   :10       NA's   :29        NA's   :69      \n       PG               REC               UB               W        \n Min.   :-2.9050   Min.   : 0.000   Min.   : 13.25   Min.   : 0.00  \n 1st Qu.: 0.3595   1st Qu.: 5.905   1st Qu.: 42.23   1st Qu.:15.29  \n Median : 1.0557   Median :18.780   Median : 60.77   Median :21.66  \n Mean   : 1.1383   Mean   :28.706   Mean   : 60.39   Mean   :22.72  \n 3rd Qu.: 1.9539   3rd Qu.:46.343   3rd Qu.: 79.90   3rd Qu.:29.16  \n Max.   : 3.9314   Max.   :96.240   Max.   :100.00   Max.   :61.25  \n                   NA's   :50       NA's   :2        NA's   :24     \n\nreg1 <- lm(CO2 ~ AE + AG + EX + GDP + I + ME + PG + REC + UB + W, data = df_wide)\n# summary(reg1)\n\n#install.packages(\"stargazer\")\n\nstargazer(reg1, type = 'html')\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nCO2\n\n\n\n\n\n\n\n\nAE\n\n\n-0.014\n\n\n\n\n\n\n(0.018)\n\n\n\n\n\n\n\n\n\n\nAG\n\n\n-0.035\n\n\n\n\n\n\n(0.041)\n\n\n\n\n\n\n\n\n\n\nEX\n\n\n0.003\n\n\n\n\n\n\n(0.011)\n\n\n\n\n\n\n\n\n\n\nGDP\n\n\n0.0001***\n\n\n\n\n\n\n(0.00002)\n\n\n\n\n\n\n\n\n\n\nI\n\n\n-0.006\n\n\n\n\n\n\n(0.019)\n\n\n\n\n\n\n\n\n\n\nME\n\n\n0.737***\n\n\n\n\n\n\n(0.189)\n\n\n\n\n\n\n\n\n\n\nPG\n\n\n-0.023\n\n\n\n\n\n\n(0.265)\n\n\n\n\n\n\n\n\n\n\nREC\n\n\n-0.055***\n\n\n\n\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\nUB\n\n\n0.019\n\n\n\n\n\n\n(0.017)\n\n\n\n\n\n\n\n\n\n\nW\n\n\n-0.040*\n\n\n\n\n\n\n(0.021)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n4.472*\n\n\n\n\n\n\n(2.566)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n138\n\n\n\n\nR2\n\n\n0.648\n\n\n\n\nAdjusted R2\n\n\n0.620\n\n\n\n\nResidual Std. Error\n\n\n2.658 (df = 127)\n\n\n\n\nF Statistic\n\n\n23.394*** (df = 10; 127)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01"
  },
  {
    "objectID": "project website.html",
    "href": "project website.html",
    "title": "NCAA Basketball Regression",
    "section": "",
    "text": "Introduction\n\nWhy does your research project matter\nMotive for project\nSummarize the project in the last paragraph\n\n\n\nData\n\nSummary statistics (Use skimr::skim())\n\n\n\nConceptual Framework\n\nJustify your ML model.\nWhy the model considers the variables in the model.\n\n\n\nModel\n\nAdd some model equation\n\nEmpirical Model:\n\\[y_{i} = \\beta_{0} + \\beta_{1}ADJOE + \\beta_{2}ADJDE + \\beta_{3}EFG_O + \\beta_{4}{EFG\\_D}_{i} + \\beta_{5}TOR_{i} + \\beta_{6}TORD_{i} + \\beta_{7}2{P\\_O}_{i} + \\beta_{8}2{P\\_D}_{i} + \\beta_{9}3{P\\_O}_{i} + \\beta_{10}SEED_{i} + \\beta_{11}WAB_{i} +  \\epsilon_{i}\\]\n\\[y_{i} = \\beta_{0} + \\beta_{1}X_{i} + \\beta_{2}X_{2i} +  \\epsilon_{i}\\] ##Creating the model in Python\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.3\n\n\nCode\ndf <- read_excel(\"C:\\\\Users\\\\Devon DiMatteo\\\\Desktop\\\\danl310df.xlsx\")\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n\nThe packages that are used to create the model are numpy, pandas, and statsmodels.apu The first step in the code is to pull the variables that will be used in the regression.\n\n\nCode\ndata=pd.read_excel(\"C:\\\\Users\\\\Devon DiMatteo\\\\Desktop\\\\danl310df.xlsx\")\ncbb_data=data.copy()\ncbb = cbb_data[[ \"W\", \"ADJOE\", \"ADJDE\", \"EFG_O\", \"EFG_D\", \"TOR\", \"TORD\", \"2P_O\",\"2P_D\", \"3P_O\",\"3P_D\", \"SEED\", \"WAB\" ]]\ncbb\n\n\n     W  ADJOE  ADJDE  EFG_O  EFG_D   TOR  ...  2P_O  2P_D  3P_O  3P_D  SEED   WAB\n0   29  121.0   89.7   54.7   44.0  19.3  ...  52.0  43.2  40.3  30.4     1   7.8\n1   31  118.9   90.2   54.9   44.9  17.2  ...  55.0  42.1  36.5  32.9     1   7.6\n2   31  111.6   86.2   53.3   41.5  20.3  ...  52.9  39.3  36.4  30.3     1   7.5\n3   35  115.9   84.5   50.6   44.8  18.3  ...  50.8  43.4  33.3  31.8     1   9.0\n4   25  107.6   85.0   51.1   43.0  20.1  ...  50.2  41.4  35.3  30.7     2   6.6\n..  ..    ...    ...    ...    ...   ...  ...   ...   ...   ...   ...   ...   ...\n63  19   90.6   95.7   45.7   45.2  22.7  ...  45.4  43.4  30.7  32.0    16 -11.5\n64  20   95.9   96.7   49.1   41.9  16.8  ...  46.0  40.5  36.7  30.0    16  -5.7\n65  20  108.1  111.2   54.4   52.6  20.3  ...  53.1  51.1  38.0  37.2    16  -8.6\n66  21   98.5   99.4   47.6   48.7  17.2  ...  46.2  46.9  33.6  34.7    16  -8.9\n67  12  100.1  109.7   48.5   47.8  19.8  ...  44.9  48.6  37.0  30.9    16 -15.6\n\n[68 rows x 13 columns]\n\n\nNext we split the features and the labels into different datasets using the .drop function\n\n\nCode\ncbb_features=cbb.drop(\"SEED\", axis=\"columns\")\ncbb_labels=cbb[\"SEED\"]\n\n\n###Linear Regression Model\nFor the linear regression model we use scikit-learn software to run the machine learning model.\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nlin_reg=LinearRegression()\nlin_reg.fit(cbb_features, cbb_labels)\n\n\nLinearRegression()\n\n\n###Decision Tree Model\nWe also use the scikit-learn software for the dicision tree regression\nThe following code is training the model:\n\n\nCode\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg=DecisionTreeRegressor()\ntree_reg.fit(cbb_features, cbb_labels)\n\n\nDecisionTreeRegressor()\n\n\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\ncbb_predictions2=tree_reg.predict(cbb_features)\ntree_mse=mean_squared_error(cbb_labels, cbb_predictions2)\n\nimport numpy as np\ntree_rmse=np.sqrt(tree_mse)\n\nprint(tree_rmse)\n\n\n0.0\n\n\n\n\nResults\n\nLinear Regression Coefficents\n\n\nCode\nlin_reg.coef_\n\n\narray([-0.05422349, -0.16885729,  0.12692155, -0.97481597, -2.58337805,\n        0.01646713, -0.01012986,  0.74896638,  1.85055441,  0.53721335,\n        1.42533091, -0.54543421])\n\n\n\n\nLinear Regression p-values\n\n\nCode\nX = sm.add_constant(cbb_features)  # add a constant term to the independent variables\nresults = sm.OLS(cbb_labels, cbb_features).fit()\np_values = results.pvalues\n\n# print the coefficients and p-values\nprint(\"Coefficients:\", lin_reg.coef_)\n\n\nCoefficients: [-0.05422349 -0.16885729  0.12692155 -0.97481597 -2.58337805  0.01646713\n -0.01012986  0.74896638  1.85055441  0.53721335  1.42533091 -0.54543421]\n\n\nCode\nprint(\"P-values:\", p_values[0:])\n\n\nP-values: W        0.664765\nADJOE    0.057468\nADJDE    0.341998\nEFG_O    0.520503\nEFG_D    0.065054\nTOR      0.899638\nTORD     0.946054\n2P_O     0.474402\n2P_D     0.052480\n3P_O     0.469688\n3P_D     0.043816\nWAB      0.002824\ndtype: float64\n\n\n\nsummarize the main results of the model you want to focus on\npridected outcomes\nWhich model performed the best\nMean squared errors residual plots\n\n\n\n\nDiscussion\n\nDiscuss the implication of your model result\nYou can make suggestions for strategy\n\n\n\nConclusion\n\nSummarize the project\nMention future work\n\n#Project\n\n\nModel\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n\n\n\nCode\ndata=pd.read_excel(\"C:\\\\Users\\\\Devon DiMatteo\\\\Desktop\\\\danl310df.xlsx\")\ncbb_data=data.copy()\ncbb = cbb_data[[ \"W\", \"ADJOE\", \"ADJDE\", \"EFG_O\", \"EFG_D\", \"TOR\", \"TORD\", \"2P_O\",\"2P_D\", \"3P_O\",\"3P_D\", \"SEED\", \"WAB\" ]]\ncbb\n\n\n     W  ADJOE  ADJDE  EFG_O  EFG_D   TOR  ...  2P_O  2P_D  3P_O  3P_D  SEED   WAB\n0   29  121.0   89.7   54.7   44.0  19.3  ...  52.0  43.2  40.3  30.4     1   7.8\n1   31  118.9   90.2   54.9   44.9  17.2  ...  55.0  42.1  36.5  32.9     1   7.6\n2   31  111.6   86.2   53.3   41.5  20.3  ...  52.9  39.3  36.4  30.3     1   7.5\n3   35  115.9   84.5   50.6   44.8  18.3  ...  50.8  43.4  33.3  31.8     1   9.0\n4   25  107.6   85.0   51.1   43.0  20.1  ...  50.2  41.4  35.3  30.7     2   6.6\n..  ..    ...    ...    ...    ...   ...  ...   ...   ...   ...   ...   ...   ...\n63  19   90.6   95.7   45.7   45.2  22.7  ...  45.4  43.4  30.7  32.0    16 -11.5\n64  20   95.9   96.7   49.1   41.9  16.8  ...  46.0  40.5  36.7  30.0    16  -5.7\n65  20  108.1  111.2   54.4   52.6  20.3  ...  53.1  51.1  38.0  37.2    16  -8.6\n66  21   98.5   99.4   47.6   48.7  17.2  ...  46.2  46.9  33.6  34.7    16  -8.9\n67  12  100.1  109.7   48.5   47.8  19.8  ...  44.9  48.6  37.0  30.9    16 -15.6\n\n[68 rows x 13 columns]\n\n\n\n\nCode\ncbb_data.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 68 entries, 0 to 67\nData columns (total 23 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   TEAM        68 non-null     object \n 1   CONF        68 non-null     object \n 2   G           68 non-null     int64  \n 3   W           68 non-null     int64  \n 4   ADJOE       68 non-null     float64\n 5   ADJDE       68 non-null     float64\n 6   BARTHAG     68 non-null     float64\n 7   EFG_O       68 non-null     float64\n 8   EFG_D       68 non-null     float64\n 9   TOR         68 non-null     float64\n 10  TORD        68 non-null     float64\n 11  ORB         68 non-null     float64\n 12  DRB         68 non-null     float64\n 13  FTR         68 non-null     float64\n 14  FTRD        68 non-null     float64\n 15  2P_O        68 non-null     float64\n 16  2P_D        68 non-null     float64\n 17  3P_O        68 non-null     float64\n 18  3P_D        68 non-null     float64\n 19  ADJ_T       68 non-null     float64\n 20  WAB         68 non-null     float64\n 21  POSTSEASON  68 non-null     object \n 22  SEED        68 non-null     int64  \ndtypes: float64(17), int64(3), object(3)\nmemory usage: 12.3+ KB\n\n\n\n\nCode\ncbb.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 68 entries, 0 to 67\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   W       68 non-null     int64  \n 1   ADJOE   68 non-null     float64\n 2   ADJDE   68 non-null     float64\n 3   EFG_O   68 non-null     float64\n 4   EFG_D   68 non-null     float64\n 5   TOR     68 non-null     float64\n 6   TORD    68 non-null     float64\n 7   2P_O    68 non-null     float64\n 8   2P_D    68 non-null     float64\n 9   3P_O    68 non-null     float64\n 10  3P_D    68 non-null     float64\n 11  SEED    68 non-null     int64  \n 12  WAB     68 non-null     float64\ndtypes: float64(11), int64(2)\nmemory usage: 7.0 KB\n\n\n\n\nCode\ncbb_features=cbb.drop(\"SEED\", axis=\"columns\")\ncbb_labels=cbb[\"SEED\"]\n\n\n#Linear Regression Model\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nlin_reg=LinearRegression()\nlin_reg.fit(cbb_features, cbb_labels)\n\n\nLinearRegression()\n\n\n\n\nCode\nlin_reg.coef_\n\n\narray([-0.05422349, -0.16885729,  0.12692155, -0.97481597, -2.58337805,\n        0.01646713, -0.01012986,  0.74896638,  1.85055441,  0.53721335,\n        1.42533091, -0.54543421])\n\n\n\n\nCode\nX = sm.add_constant(cbb_features)  # add a constant term to the independent variables\nresults = sm.OLS(cbb_labels, cbb_features).fit()\np_values = results.pvalues\n\n# print the coefficients and p-values\nprint(\"Coefficients:\", lin_reg.coef_)\n\n\nCoefficients: [-0.05422349 -0.16885729  0.12692155 -0.97481597 -2.58337805  0.01646713\n -0.01012986  0.74896638  1.85055441  0.53721335  1.42533091 -0.54543421]\n\n\nCode\nprint(\"P-values:\", p_values[0:])\n\n\nP-values: W        0.664765\nADJOE    0.057468\nADJDE    0.341998\nEFG_O    0.520503\nEFG_D    0.065054\nTOR      0.899638\nTORD     0.946054\n2P_O     0.474402\n2P_D     0.052480\n3P_O     0.469688\n3P_D     0.043816\nWAB      0.002824\ndtype: float64\n\n\n\n\nCode\nalpha = 0.05\nsignificant_coeffs = [i for i in range(len(p_values)) if p_values[i] < alpha]\nprint(\"Significant coefficients:\", significant_coeffs)\n\n\nSignificant coefficients: [10, 11]\n\n\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\ncbb_predictions=lin_reg.predict(cbb_features)\nlin_mse=mean_squared_error(cbb_labels,cbb_predictions)\n\nimport numpy as np\nlin_rmse=np.sqrt(lin_mse)\n\nprint(lin_rmse)\n\n\n1.9130506804113334\n\n\n#Decision Tree Model\n\n\nCode\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg=DecisionTreeRegressor()\ntree_reg.fit(cbb_features, cbb_labels)\n\n\nDecisionTreeRegressor()\n\n\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\ncbb_predictions2=tree_reg.predict(cbb_features)\ntree_mse=mean_squared_error(cbb_labels, cbb_predictions2)\n\nimport numpy as np\ntree_rmse=np.sqrt(tree_mse)\n\nprint(tree_rmse)\n\n\n0.0\n\n\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# evaluate the model using cross-validation\nscores = cross_val_score(tree_reg, cbb_features, cbb_labels, cv=5)\nprint(\"Cross-validation scores:\", scores)\n\n\nCross-validation scores: [ -4.11538462 -18.92307692 -10.17307692 -13.85714286 -19.58333333]\n\n\nCode\nprint(\"Mean score:\", scores.mean())\n# -15 is a sign of overfitting\n\n\nMean score: -13.330402930402931\n\n\n\n\nCode\nprint(\"Feature importances:\", tree_reg.feature_importances_)\n\n\nFeature importances: [8.04352969e-04 0.00000000e+00 1.24518018e-02 3.57490208e-03\n 5.50598759e-03 1.70112528e-02 2.10127057e-02 4.06238873e-03\n 4.46862761e-04 1.60870594e-03 1.05683043e-02 9.22952735e-01]\n\n\n#Random Forest\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nforest_reg=RandomForestRegressor()\nforest_reg.fit(cbb_features,cbb_labels)\n\n\nRandomForestRegressor()\n\n\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\ncbb_predictions3=forest_reg.predict(cbb_features)\nforest_mse=mean_squared_error(cbb_labels, cbb_predictions3)\nforest_rmse=np.sqrt(forest_mse)\n\n\n\n\nCode\nprint(forest_rmse)\n\n\n0.7017226702069365\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# evaluate the model using cross-validation\nscores = cross_val_score(forest_reg, cbb_features, cbb_labels, cv=5)\nprint(\"Cross-validation scores:\", scores)\n\n\nCross-validation scores: [ -8.52778077 -10.09440096  -3.36011827 -10.437335   -14.4115    ]\n\n\nCode\nprint(\"Mean score:\", scores.mean())\n\n\nMean score: -9.366227\n\n\n\n\nCode\nprint(\"Feature importances:\", forest_reg.feature_importances_)\n\n\nFeature importances: [0.00839758 0.00646017 0.01306566 0.00476062 0.00769221 0.01360642\n 0.00704066 0.00385078 0.00593136 0.00610077 0.00679039 0.91630337]"
  },
  {
    "objectID": "DANL310_hw2_DiMatteo_Devon.html",
    "href": "DANL310_hw2_DiMatteo_Devon.html",
    "title": "Homework 2",
    "section": "",
    "text": "#2A\nhdi_corruption <- read_csv(\n  'https://bcdanl.github.io/data/hdi_corruption.csv')\ndf1 <- hdi_corruption %>% \n  group_by(region)\np1 <- ggplot(data = df1, aes(y = hdi, x = cpi), color = region)+\n  geom_point()+\n  labs(x = \"Corruption Perception Index, 2014 (100 = least corrupt)\", \n       y = \"Human Development Index, 2014 (1.0 = most developed\")\np1 + geom_smooth()\n\n\n\np1\n\n\n\n\n\n#2B\n\n# library(readr)\nlabor_supply <- read_csv(\"C:/Users/Devon DiMatteo/Downloads/labor_supply (1).zip\")\ndf <- labor_supply %>% \n  group_by(YEAR) %>% \n  mutate(pop = sum(ASECWT)) %>% \n  mutate(child = ifelse(NCHLT5 < 5, \"Having a Child Under Age 5 in Household\"\n                        , \"No Child Under Age 5 in Highschool\"))\n  \np2 <- ggplot(data = df, aes(y = pop, x = YEAR))+\n  geom_line()+\n  facet_wrap(child ~ .)+\n  labs(y = \"Labor Force Participation Rate\")\np2\n\n\n\n\n\n#2C\n#install.packages(\"ggcorrplot\")\nlibrary(ggcorrplot) # to create correlation heatmaps using ggcorrplot()\n\nbeer_mkt <- read_csv('https://bcdanl.github.io/data/beer_markets.csv')\n\nbeer_dummies <- beer_mkt %>% select(-hh, -market) \nreg <- lm(data = beer_dummies,\n          beer_floz ~ .)\nbeer_dummies <-  as.data.frame(model.matrix(reg))[, -1]\nbeer_dummies <- cbind(beer_mkt$beer_floz ,beer_dummies)\nbeer_dummies <- beer_dummies %>% \n  rename(beer_floz = `beer_mkt$beer_floz`)\n\ncorrelation <- as.data.frame(cor(beer_dummies))\n\n#NY markets are\n#ALBANY, BUFFALO-ROCHESTER, RURAL NEW YORK, URBAN NY, \n#SUBURBAN NY, EXURBAN NY, SYRACUSE\n\n\np3 <- ggcorrplot(correlation)\np3"
  },
  {
    "objectID": "DANL310_midterm_DiMatteo_Devon.html",
    "href": "DANL310_midterm_DiMatteo_Devon.html",
    "title": "DANL 310: Data Visualization and Presentation Midterm Exam",
    "section": "",
    "text": "I solemnly swear that I will not cheat or engage in any form of academic dishonesty during this exam.\n\nI will not communicate with other students or use unauthorized materials.\n\nI will uphold the integrity of this exam and demonstrate my own knowledge and abilities.\n\nBy taking this pledge, I acknowledge that academic dishonesty undermines the academic process and is a violation of the trust placed in me as a student.\n\nI accept the consequences of any violation of this promise.\n\nStudentâs Signature: Devon DiMatteo\n\n\nThe midterm exam questions are provided in the following webpage: - https://bcdanl.github.io/DANL310-midterm-q.html.\n\n\n\n\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(socviz)\nlibrary(ggthemes)\nlibrary(hrbrthemes)\nlibrary(gapminder)\n\n\nhrbrthemes::theme_ipsum() or theme_minimal() can be used for your ggplot theme."
  },
  {
    "objectID": "Homework 3 Danl310.html",
    "href": "Homework 3 Danl310.html",
    "title": "Homework 3",
    "section": "",
    "text": "library(shiny)\n\nWarning: package 'shiny' was built under R version 4.2.3\n\n#install.packages(\"shinythemes\")\nlibrary(shinythemes)\n\nWarning: package 'shinythemes' was built under R version 4.2.3\n\nlibrary(tidyverse)\n\nââ Attaching packages âââââââââââââââââââââââââââââââââââââââ tidyverse 1.3.2 ââ\nâ ggplot2 3.4.1     â purrr   1.0.1\nâ tibble  3.1.8     â dplyr   1.1.0\nâ tidyr   1.3.0     â stringr 1.5.0\nâ readr   2.1.4     â forcats 1.0.0\nââ Conflicts ââââââââââââââââââââââââââââââââââââââââââ tidyverse_conflicts() ââ\nâ dplyr::filter() masks stats::filter()\nâ dplyr::lag()    masks stats::lag()\n\ndata(\"faithful\")\ndf <- faithful\n\ndf1 <- faithful %>%\n  summarize(count = n()) %>%\n  arrange(desc(count))\n\n\n#ui <- fluidPage(theme = shinytheme(\"yeti\"),\n  \n#  titlePanel(\"Old Faithful Geyser Data\"),\n  \n#  sidebarLayout(\n\n#    sidebarPanel(\n#      selectInput(\"stormStatus\",\n#                  label = \"Filter by storm type: \",\n#                  choices = unique(stormNames$status))\n#    ),\n#    \n#    mainPanel(\n#      p(\"Histogram of Waiting Times\"),\n#      plotOutput(\"nameDist\"),\n#      \n#    )\n#  )\n#)\n\n#server <- function(input, output, session) {\n  \n  \n#  output$nameDist <- renderPlot({\n    \n#    str(input$stormStatus)\n    \n#    names_filtered <- filter(stormNames, status == input$stormStatus)\n#    hist(names_filtered$count)\n\n#  },\n#  width = 400, height = 300)\n  \n#  output$nameTable <- renderTable({\n    \n#    names_filtered <- filter(stormNames, status == input$stormStatus)\n#    head(names_filtered, 20)\n    \n#  })\n  \n#}\n\n#shinyApp(ui, server)"
  }
]